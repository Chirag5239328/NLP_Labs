{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T14:06:34.633499Z","iopub.execute_input":"2024-09-01T14:06:34.634016Z","iopub.status.idle":"2024-09-01T14:06:34.995165Z","shell.execute_reply.started":"2024-09-01T14:06:34.633981Z","shell.execute_reply":"2024-09-01T14:06:34.994326Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nsquad = load_dataset(\"squad\", split=\"train[:5000]\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:30:01.393607Z","iopub.execute_input":"2024-09-01T14:30:01.394495Z","iopub.status.idle":"2024-09-01T14:30:06.596180Z","shell.execute_reply.started":"2024-09-01T14:30:01.394439Z","shell.execute_reply":"2024-09-01T14:30:06.595436Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f1cef66f1345ef8e6eaed8af1f7bcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e90f3c3e0a4e4add8fcf9ab864729799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15263b69d9be425593f7206b5e195328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a58793d1eb5491abf548564599d37dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a2bde8854640268d09775aafc8608b"}},"metadata":{}}]},{"cell_type":"code","source":"squad = squad.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:30:06.597773Z","iopub.execute_input":"2024-09-01T14:30:06.598109Z","iopub.status.idle":"2024-09-01T14:30:06.615501Z","shell.execute_reply.started":"2024-09-01T14:30:06.598071Z","shell.execute_reply":"2024-09-01T14:30:06.614620Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:30:45.272054Z","iopub.execute_input":"2024-09-01T14:30:45.272430Z","iopub.status.idle":"2024-09-01T14:30:46.552479Z","shell.execute_reply.started":"2024-09-01T14:30:45.272397Z","shell.execute_reply":"2024-09-01T14:30:46.551525Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a883d26339fd449fadb6943a3b6e1b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4c923f652a4c51990eed90ac2949a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60edb7189f24aa597a539a6b493a414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c87e5db731c4546a452d936a11dfb89"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        answer = answers[i]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label it (0, 0)\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:31:34.708437Z","iopub.execute_input":"2024-09-01T14:31:34.709382Z","iopub.status.idle":"2024-09-01T14:31:34.719609Z","shell.execute_reply.started":"2024-09-01T14:31:34.709337Z","shell.execute_reply":"2024-09-01T14:31:34.718713Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:32:07.750015Z","iopub.execute_input":"2024-09-01T14:32:07.750392Z","iopub.status.idle":"2024-09-01T14:32:10.731127Z","shell.execute_reply.started":"2024-09-01T14:32:07.750355Z","shell.execute_reply":"2024-09-01T14:32:10.730108Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4ae36548d84f1b9f7403013c14e2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5ab7bea337428494dda993476fd752"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:33:38.514476Z","iopub.execute_input":"2024-09-01T14:33:38.514893Z","iopub.status.idle":"2024-09-01T14:33:38.519461Z","shell.execute_reply.started":"2024-09-01T14:33:38.514854Z","shell.execute_reply":"2024-09-01T14:33:38.518541Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:33:46.669510Z","iopub.execute_input":"2024-09-01T14:33:46.669931Z","iopub.status.idle":"2024-09-01T14:33:47.954917Z","shell.execute_reply.started":"2024-09-01T14:33:46.669893Z","shell.execute_reply":"2024-09-01T14:33:47.953968Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e422f71186e540cb9437cb2e5b85d2c5"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable WandB logging\n\ntraining_args = TrainingArguments(\n    output_dir=\"./my_awesome_qa_model\",\n    eval_strategy=\"epoch\",  # Updated to eval_strategy\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",   # Optional for logging\n    save_total_limit=1\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_squad[\"train\"],\n    eval_dataset=tokenized_squad[\"test\"],  # Use validation set for evaluation\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics  # Pass the custom metric function here\n)\n\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:46:03.932318Z","iopub.execute_input":"2024-09-01T14:46:03.933206Z","iopub.status.idle":"2024-09-01T14:53:26.887915Z","shell.execute_reply.started":"2024-09-01T14:46:03.933164Z","shell.execute_reply":"2024-09-01T14:53:26.886963Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 07:21, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Token Level Iou</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.630697</td>\n      <td>0.526942</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.040400</td>\n      <td>1.729835</td>\n      <td>0.542635</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.040400</td>\n      <td>1.671823</td>\n      <td>0.548313</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.586200</td>\n      <td>1.825051</td>\n      <td>0.546232</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.586200</td>\n      <td>1.871797</td>\n      <td>0.550826</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1250, training_loss=0.7386638549804687, metrics={'train_runtime': 442.2205, 'train_samples_per_second': 45.226, 'train_steps_per_second': 2.827, 'total_flos': 1959796500480000.0, 'train_loss': 0.7386638549804687, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:53:35.769312Z","iopub.execute_input":"2024-09-01T14:53:35.770055Z","iopub.status.idle":"2024-09-01T14:53:43.025452Z","shell.execute_reply.started":"2024-09-01T14:53:35.770013Z","shell.execute_reply":"2024-09-01T14:53:43.024505Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='126' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 1.8717974424362183, 'eval_token_level_iou': 0.5508263698992213, 'eval_runtime': 7.2469, 'eval_samples_per_second': 137.989, 'eval_steps_per_second': 8.693, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_token_level_iou(pred_start, pred_end, true_start, true_end):\n    pred_tokens = set(range(pred_start, pred_end + 1))\n    true_tokens = set(range(true_start, true_end + 1))\n\n    intersection = len(pred_tokens.intersection(true_tokens))\n    union = len(pred_tokens.union(true_tokens))\n\n    iou = intersection / union if union != 0 else 0\n    return iou\n\ndef compute_metrics(eval_pred):\n    start_preds, end_preds = eval_pred.predictions\n    start_labels, end_labels = eval_pred.label_ids\n\n    ious = []\n    for i in range(len(start_preds)):\n        iou = compute_token_level_iou(\n            start_preds[i].argmax(), end_preds[i].argmax(),\n            start_labels[i], end_labels[i]\n        )\n        ious.append(iou)\n\n    mean_iou = sum(ious) / len(ious)\n    return {\"token_level_iou\": mean_iou}\n\n# Re-run evaluation with the custom metric\neval_results = trainer.evaluate(metric_key_prefix=\"eval\")\nprint(f\"Evaluation results with custom metric: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:53:56.147752Z","iopub.execute_input":"2024-09-01T14:53:56.148719Z","iopub.status.idle":"2024-09-01T14:54:03.389705Z","shell.execute_reply.started":"2024-09-01T14:53:56.148641Z","shell.execute_reply":"2024-09-01T14:54:03.388688Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Evaluation results with custom metric: {'eval_loss': 1.8717974424362183, 'eval_token_level_iou': 0.5508263698992213, 'eval_runtime': 7.2289, 'eval_samples_per_second': 138.335, 'eval_steps_per_second': 8.715, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"def answer_question(question, context):\n    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n\n    # Move inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n    answer_start = torch.argmax(outputs.start_logits)\n    answer_end = torch.argmax(outputs.end_logits) + 1\n\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n    return answer\n\n\n# Example usage:\nquestion = \"What is the name of the repository?\"\ncontext = \"The Hugging Face library provides an easy way to use transformer models.\"\nanswer = answer_question(question, context)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:55:39.414645Z","iopub.execute_input":"2024-09-01T14:55:39.415044Z","iopub.status.idle":"2024-09-01T14:55:39.444790Z","shell.execute_reply.started":"2024-09-01T14:55:39.415008Z","shell.execute_reply":"2024-09-01T14:55:39.443816Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Question: What is the name of the repository?\nAnswer: hugging face library\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available and set the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the correct device\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:57:54.744955Z","iopub.execute_input":"2024-09-01T14:57:54.745332Z","iopub.status.idle":"2024-09-01T14:57:54.837430Z","shell.execute_reply.started":"2024-09-01T14:57:54.745295Z","shell.execute_reply":"2024-09-01T14:57:54.836458Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./my_awesome_qa_model\")\ntokenizer.save_pretrained(\"./my_awesome_qa_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:57:47.065172Z","iopub.execute_input":"2024-09-01T14:57:47.065822Z","iopub.status.idle":"2024-09-01T14:57:47.790453Z","shell.execute_reply.started":"2024-09-01T14:57:47.065781Z","shell.execute_reply":"2024-09-01T14:57:47.789596Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"('./my_awesome_qa_model/tokenizer_config.json',\n './my_awesome_qa_model/special_tokens_map.json',\n './my_awesome_qa_model/vocab.txt',\n './my_awesome_qa_model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n\n# Reload the model and tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"./my_awesome_qa_model\")\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"./my_awesome_qa_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T14:57:49.295272Z","iopub.execute_input":"2024-09-01T14:57:49.296169Z","iopub.status.idle":"2024-09-01T14:57:49.378630Z","shell.execute_reply.started":"2024-09-01T14:57:49.296127Z","shell.execute_reply":"2024-09-01T14:57:49.377904Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\n\n# Testing the inference pipeline again\nquestion = \"Which country contains the majority of the Amazon rainforest?\"\ncontext = \"The Amazon rainforest is the largest tropical rainforest in the world, with 60% of it located in Brazil.\"\nanswer = answer_question(question, context)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:01:02.349042Z","iopub.execute_input":"2024-09-01T15:01:02.349710Z","iopub.status.idle":"2024-09-01T15:01:02.366491Z","shell.execute_reply.started":"2024-09-01T15:01:02.349669Z","shell.execute_reply":"2024-09-01T15:01:02.365641Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Question: Which country contains the majority of the Amazon rainforest?\nAnswer: brazil\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}